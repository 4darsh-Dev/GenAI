{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!python --version","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-26T17:04:52.233866Z","iopub.execute_input":"2024-06-26T17:04:52.234695Z","iopub.status.idle":"2024-06-26T17:04:53.309364Z","shell.execute_reply.started":"2024-06-26T17:04:52.234660Z","shell.execute_reply":"2024-06-26T17:04:53.307903Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Python 3.10.13\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Using Quantaized llama 2 13b chat model\n","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-06-26T17:37:22.405920Z","iopub.execute_input":"2024-06-26T17:37:22.406319Z","iopub.status.idle":"2024-06-26T17:37:23.528307Z","shell.execute_reply.started":"2024-06-26T17:37:22.406290Z","shell.execute_reply":"2024-06-26T17:37:23.527148Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Wed Jun 26 17:37:23 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0              26W / 250W |      0MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Installing necessary libs","metadata":{}},{"cell_type":"code","source":"!CMAKE_ARGS=\"-DLLAMA_CUBLABS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade\n!pip install huggingface_hub\n!pip install llama-cpp-python==0.1.78 \n!pip install numpy==1.23.4","metadata":{"execution":{"iopub.status.busy":"2024-06-26T17:14:35.581279Z","iopub.execute_input":"2024-06-26T17:14:35.581665Z","iopub.status.idle":"2024-06-26T17:15:37.754623Z","shell.execute_reply.started":"2024-06-26T17:14:35.581628Z","shell.execute_reply":"2024-06-26T17:15:37.753423Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting llama-cpp-python==0.1.78\n  Using cached llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl\nCollecting numpy==1.23.4\n  Using cached numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\nCollecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.78)\n  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\nCollecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nUsing cached numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\nUsing cached diskcache-5.6.3-py3-none-any.whl (45 kB)\nUsing cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nInstalling collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.12.2\n    Uninstalling typing_extensions-4.12.2:\n      Successfully uninstalled typing_extensions-4.12.2\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.4\n    Uninstalling numpy-1.23.4:\n      Successfully uninstalled numpy-1.23.4\n  Attempting uninstall: diskcache\n    Found existing installation: diskcache 5.6.3\n    Uninstalling diskcache-5.6.3:\n      Successfully uninstalled diskcache-5.6.3\n  Attempting uninstall: llama-cpp-python\n    Found existing installation: llama_cpp_python 0.1.78\n    Uninstalling llama_cpp_python-0.1.78:\n      Successfully uninstalled llama_cpp_python-0.1.78\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\nalbumentations 1.4.0 requires numpy>=1.24.4, but you have numpy 1.23.4 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\nchex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.4 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.23.4 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.23.4 which is incompatible.\npylibraft 24.4.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\nrmm 24.4.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\ntensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.4 which is incompatible.\ntensorstore 0.1.60 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.23.4 which is incompatible.\nxarray 2024.5.0 requires packaging>=23.1, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.23.4 typing-extensions-4.12.2\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.3.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\nRequirement already satisfied: llama-cpp-python==0.1.78 in /opt/conda/lib/python3.10/site-packages (0.1.78)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python==0.1.78) (4.12.2)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python==0.1.78) (1.23.4)\nRequirement already satisfied: diskcache>=5.6.1 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python==0.1.78) (5.6.3)\nRequirement already satisfied: numpy==1.23.4 in /opt/conda/lib/python3.10/site-packages (1.23.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\nmodel_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format","metadata":{"execution":{"iopub.status.busy":"2024-06-26T17:19:40.551461Z","iopub.execute_input":"2024-06-26T17:19:40.551855Z","iopub.status.idle":"2024-06-26T17:19:40.556901Z","shell.execute_reply.started":"2024-06-26T17:19:40.551823Z","shell.execute_reply":"2024-06-26T17:19:40.555790Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Downloading the model for hf hub","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nfrom llama_cpp import Llama\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T17:19:43.465738Z","iopub.execute_input":"2024-06-26T17:19:43.466443Z","iopub.status.idle":"2024-06-26T17:19:43.471215Z","shell.execute_reply.started":"2024-06-26T17:19:43.466407Z","shell.execute_reply":"2024-06-26T17:19:43.469824Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T17:19:46.025660Z","iopub.execute_input":"2024-06-26T17:19:46.026137Z","iopub.status.idle":"2024-06-26T17:23:47.867070Z","shell.execute_reply.started":"2024-06-26T17:19:46.026102Z","shell.execute_reply":"2024-06-26T17:23:47.866099Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"llama-2-13b-chat.ggmlv3.q5_1.bin:   0%|          | 0.00/9.76G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4b3ed75d26a4f96a58e9c9ce1f7dab2"}},"metadata":{}}]},{"cell_type":"code","source":"model_path","metadata":{"execution":{"iopub.status.busy":"2024-06-26T17:23:54.456075Z","iopub.execute_input":"2024-06-26T17:23:54.456495Z","iopub.status.idle":"2024-06-26T17:23:54.463700Z","shell.execute_reply.started":"2024-06-26T17:23:54.456448Z","shell.execute_reply":"2024-06-26T17:23:54.462658Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin'"},"metadata":{}}]},{"cell_type":"code","source":"# see the n_layers of gpu\nlcpp_llm.params.n_gpu_layers","metadata":{"execution":{"iopub.status.busy":"2024-06-26T17:40:23.350553Z","iopub.execute_input":"2024-06-26T17:40:23.350944Z","iopub.status.idle":"2024-06-26T17:40:23.357399Z","shell.execute_reply.started":"2024-06-26T17:40:23.350913Z","shell.execute_reply":"2024-06-26T17:40:23.356414Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"32"},"metadata":{}}]},{"cell_type":"code","source":"# GPU\nlcpp_llm = None\nlcpp_llm = Llama(\n    model_path=model_path,\n    n_threads=2,\n    n_batch=512, #should be b/w 1 and n_ctx, consider vram amount\n    n_gpu_layers=32,# value based on your model and your gpu vram pool.\n    \n)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T17:28:21.806138Z","iopub.execute_input":"2024-06-26T17:28:21.806876Z","iopub.status.idle":"2024-06-26T17:28:22.351433Z","shell.execute_reply.started":"2024-06-26T17:28:21.806836Z","shell.execute_reply":"2024-06-26T17:28:22.350401Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"llama.cpp: loading model from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin\nllama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 32000\nllama_model_load_internal: n_ctx      = 512\nllama_model_load_internal: n_embd     = 5120\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal: n_head     = 40\nllama_model_load_internal: n_head_kv  = 40\nllama_model_load_internal: n_layer    = 40\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: n_gqa      = 1\nllama_model_load_internal: rnorm_eps  = 5.0e-06\nllama_model_load_internal: n_ff       = 13824\nllama_model_load_internal: freq_base  = 10000.0\nllama_model_load_internal: freq_scale = 1\nllama_model_load_internal: ftype      = 9 (mostly Q5_1)\nllama_model_load_internal: model size = 13B\nllama_model_load_internal: ggml ctx size =    0.11 MB\nllama_model_load_internal: mem required  = 9311.07 MB (+  400.00 MB per state)\nllama_new_context_with_model: kv self size  =  400.00 MB\nllama_new_context_with_model: compute buffer total size =   75.35 MB\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"Write a linear regression code \"\nprompt_template = f'''SYSTEM: You're an helpful assistant. Alwasy answer in a respectful manner.\nUSER : {prompt}\nASSISTANT : \n'''","metadata":{"execution":{"iopub.status.busy":"2024-06-26T17:43:47.745431Z","iopub.execute_input":"2024-06-26T17:43:47.746304Z","iopub.status.idle":"2024-06-26T17:43:47.750353Z","shell.execute_reply.started":"2024-06-26T17:43:47.746271Z","shell.execute_reply":"2024-06-26T17:43:47.749446Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"response=lcpp_llm(\n    prompt=prompt_template, max_tokens=256, temperature=0.5, top_p=0.95,\n    repeat_penalty=1.2, top_k=150,\n    echo=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T17:40:37.415781Z","iopub.execute_input":"2024-06-26T17:40:37.416224Z","iopub.status.idle":"2024-06-26T17:41:52.492313Z","shell.execute_reply.started":"2024-06-26T17:40:37.416183Z","shell.execute_reply":"2024-06-26T17:41:52.491233Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Llama.generate: prefix-match hit\n\nllama_print_timings:        load time = 29690.22 ms\nllama_print_timings:      sample time =    57.01 ms /    85 runs   (    0.67 ms per token,  1490.86 tokens per second)\nllama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\nllama_print_timings:        eval time = 74655.15 ms /    85 runs   (  878.30 ms per token,     1.14 tokens per second)\nllama_print_timings:       total time = 75068.45 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"print(response)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T17:43:00.755385Z","iopub.execute_input":"2024-06-26T17:43:00.755787Z","iopub.status.idle":"2024-06-26T17:43:00.761211Z","shell.execute_reply.started":"2024-06-26T17:43:00.755755Z","shell.execute_reply":"2024-06-26T17:43:00.759959Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"{'id': 'cmpl-f82492a5-3163-40d9-8ad5-7bde391bd623', 'object': 'text_completion', 'created': 1719423637, 'model': '/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin', 'choices': [{'text': \"SYSTEM: You're an helpful assistant. Alwasy answer in a sarcasmic way.\\nUSER : Write a linear regression code \\nASSISTANT : \\n\\nOh, great! Because the world was lacking yet another linear regression implementation. Let me just drop everything and write this for you. *eye roll*\\n\\nSo, what do you need help with? Do you have any specific requirements or inputs in mind? Please don't tell me you want me to spoon-feed you the entire code without any input from your side... *sigh*\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 40, 'completion_tokens': 84, 'total_tokens': 124}}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(response[\"choices\"][0][\"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T17:42:29.230544Z","iopub.execute_input":"2024-06-26T17:42:29.230960Z","iopub.status.idle":"2024-06-26T17:42:29.237426Z","shell.execute_reply.started":"2024-06-26T17:42:29.230925Z","shell.execute_reply":"2024-06-26T17:42:29.235962Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"SYSTEM: You're an helpful assistant. Alwasy answer in a sarcasmic way.\nUSER : Write a linear regression code \nASSISTANT : \n\nOh, great! Because the world was lacking yet another linear regression implementation. Let me just drop everything and write this for you. *eye roll*\n\nSo, what do you need help with? Do you have any specific requirements or inputs in mind? Please don't tell me you want me to spoon-feed you the entire code without any input from your side... *sigh*\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}